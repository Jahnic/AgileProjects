{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01b3ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a91f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "source_folder = \"../data/interim/\"\n",
    "\n",
    "# define tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenizer max sentance lenth\n",
    "max_input_length = 100\n",
    "\n",
    "# initialize pretrained model\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "96a93501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [batch size, sent len] \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]       \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])      \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf944f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTGRUSentiment(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (rnn): GRU(768, 256, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "  (out): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.6\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "model.load_state_dict(torch.load(\"../models/binary_transformer_model.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ad512f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../data/interim/test_bin_data.csv\")\n",
    "test_labels = test_df[\"1\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "306d6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_predict(sentence):\n",
    "    '''\n",
    "    Tokenizes a sentances, removes two characters from maximum length (282) and \n",
    "    converts tokens to corresponding embedding index\n",
    "    \n",
    "    Returns: embedding indices tensor of tokens in sentence\n",
    "    '''\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, return_tensors=\"pt\")\n",
    "    indicies_tensor = inputs[\"input_ids\"]\n",
    "    return indicies_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9f7fb065",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sentence in test_df[\"10\"]:\n",
    "    indices = tokenize_cut_embed(sentence)\n",
    "    pred = model(indices)\n",
    "    pred = torch.round(torch.sigmoid(pred))\n",
    "    predictions.append(float(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "47c6e77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.921895343843184\n"
     ]
    }
   ],
   "source": [
    "test_f1 = f1_score(test_labels, predictions, average=\"weighted\")\n",
    "print(\"Test F1:\", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "57e8b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated. Could have humans label some to get a better estimate. Will be our estimate of Bayes Error\n",
    "human_f1 = 0.99 \n",
    "train_f1 = 0.82\n",
    "validation_f1 = 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9ac636df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=[\"Human\", \"Train\", \"Validation\", \"Test\"], y=[human_f1, train_f1, validation_f1, test_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "546d9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "avoidable_bias = human_f1 - train_f1\n",
    "variance = train_f1 - validation_f1\n",
    "overfit_to_validation = validation_f1 - test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5531079a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.921895343843184\n",
      "Avoidable Bias: 0.17000000000000004\n",
      "Variance: 0.09999999999999998\n",
      "Validation F1 - Test F1: -0.20189534384318397\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test F1: {test_f1}\")\n",
    "print(f\"Avoidable Bias: {avoidable_bias}\")\n",
    "print(f\"Variance: {variance}\")\n",
    "print(f\"Validation F1 - Test F1: {overfit_to_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f14a73b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96      1243\n",
      "           1       0.80      0.67      0.73       221\n",
      "\n",
      "    accuracy                           0.92      1464\n",
      "   macro avg       0.87      0.82      0.84      1464\n",
      "weighted avg       0.92      0.92      0.92      1464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8b0fbbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter_sentiment",
   "language": "python",
   "name": "twitter_sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
