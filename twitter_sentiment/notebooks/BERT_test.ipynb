{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01b3ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a91f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "source_folder = \"../data/interim/\"\n",
    "\n",
    "# define tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenizer max sentance lenth\n",
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "# initialize pretrained model\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96a93501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [batch size, sent len] \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]       \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])      \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf944f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTGRUSentiment(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (rnn): GRU(768, 256, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "  (out): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "model.load_state_dict(torch.load(\"../models/binary_transformer_model.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad512f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../data/interim/test_bin_data.csv\")\n",
    "test_labels = test_df[\"1\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "306d6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_cut_embed(sentence):\n",
    "    '''\n",
    "    Tokenizes a sentances, removes two characters from maximum length (282) and \n",
    "    converts tokens to corresponding embedding index\n",
    "    \n",
    "    Returns: embedding indices tensor of tokens in sentence\n",
    "    '''\n",
    "    \n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    embed = tokenizer.convert_tokens_to_ids\n",
    "    indicies = [embed(token) for token in sentence]\n",
    "    indicies_tensor = torch.LongTensor(indicies).view(1,-1).to(device)\n",
    "    return indicies_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f7fb065",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sentence in test_df[\"10\"]:\n",
    "    indices = tokenize_cut_embed(sentence)\n",
    "    pred = model(indices)\n",
    "    pred = torch.round(torch.sigmoid(pred))\n",
    "    predictions.append(float(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47c6e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f1 = f1_score(test_labels, predictions, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57e8b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated. Could have humans label some to get a better estimate. Will be our estimate of Bayes Error\n",
    "human_f1 = 0.99 \n",
    "train_f1 = 0.82\n",
    "validation_f1 = 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ac636df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=[\"Human\", \"Train\", \"Validation\", \"Test\"], y=[human_f1, train_f1, validation_f1, test_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "546d9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "avoidable_bias = human_f1 - train_f1\n",
    "variance = train_f1 - validation_f1\n",
    "overfit_to_validation = validation_f1 - test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5531079a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.7786062060172868\n",
      "Avoidable Bias: 0.17000000000000004\n",
      "Variance: 0.09999999999999998\n",
      "Validation F1 - Test F1: -0.05860620601728683\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test F1: {test_f1}\")\n",
    "print(f\"Avoidable Bias: {avoidable_bias}\")\n",
    "print(f\"Variance: {variance}\")\n",
    "print(f\"Validation F1 - Test F1: {overfit_to_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "908e483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.92      1243\n",
      "           1       0.11      0.00      0.01       221\n",
      "\n",
      "    accuracy                           0.84      1464\n",
      "   macro avg       0.48      0.50      0.46      1464\n",
      "weighted avg       0.74      0.84      0.78      1464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51fe1a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter_sentiment",
   "language": "python",
   "name": "twitter_sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
